{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ef98c0f",
   "metadata": {},
   "source": [
    "# Estudo NLTK e Gensim\n",
    "\n",
    "Notebook baseado no estudo do Homero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d56abba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Gemsin\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# SkLearn\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# XBGBoost\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec8eb255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk.__version__ 3.7\n",
      "sklearn.__version__ 1.0.2\n",
      "gensim.__version__ 4.3.0\n",
      "xgboost.__version__ 1.7.5\n",
      "tensorflow.__version__ 2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(\"nltk.__version__\", nltk.__version__)\n",
    "print(\"sklearn.__version__\", sklearn.__version__)\n",
    "print(\"gensim.__version__\", gensim.__version__)\n",
    "print(\"xgboost.__version__\", xgb.__version__)\n",
    "print(\"tensorflow.__version__\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "167b96f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Eric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Eric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Eric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Eric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Eric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download das bases do NLTK que serão utilizadas\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93302d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df1e08d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para armazenar documentos e labels\n",
    "documents = []\n",
    "\n",
    "# Iterar sobre as categorias\n",
    "for category in movie_reviews.categories():\n",
    "  for fileid in movie_reviews.fileids(category):\n",
    "    # Obter o texto bruto do arquivo e a categoria\n",
    "    doc = movie_reviews.raw(fileid)\n",
    "    label = category\n",
    "    # Adicionar o documento e label à lista\n",
    "    documents.append((doc, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "215f164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processed_docs(documents, remove_stopwords=True):\n",
    "    # Pré-processamento\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    processed_docs = []\n",
    "    processed_sentences = []\n",
    "  \n",
    "    for doc, label in documents:\n",
    "        # Tokenização e conversão para minúsculas\n",
    "        text = doc.lower()\n",
    "        text = doc\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remoção de pontuação\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "        # Remoção de stopwords\n",
    "        if remove_stopwords:\n",
    "            tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "        # Lematização\n",
    "        # tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        # Stemmização\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "        # Adicionar documento processado e rótulo à lista\n",
    "        processed_docs.append((tokens, label))\n",
    "        processed_sentences.append(tokens)\n",
    "\n",
    "    return processed_docs, processed_sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d750373",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs, processed_sentences = pre_processed_docs(documents)\n",
    "processed_docs_complete, processed_sentences_complete = pre_processed_docs(documents, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdb861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label for doc, label in processed_docs]\n",
    "print(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels.count(\"neg\"), labels.count(\"pos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac09fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise documentos quando stopwords são removidas\n",
    "tokens = [doc for doc, label in processed_docs]\n",
    "tokens = [token for token in tokens for token in token]\n",
    "frequencia = nltk.FreqDist(tokens)\n",
    "\n",
    "print(\"Total de tokens:\", len(tokens))\n",
    "print(\"Total de tokens únicos:\", len(set(tokens)))\n",
    "print(\"Tokens mais comuns:\", frequencia.most_common(50))\n",
    "\n",
    "\n",
    "frequencia.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887ae50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise documentos quando stopwords não são removidas\n",
    "tokens = [doc for doc, label in processed_docs_complete]\n",
    "tokens = [token for token in tokens for token in token]\n",
    "frequencia = nltk.FreqDist(tokens)\n",
    "\n",
    "print(\"Total de tokens:\", len(tokens))\n",
    "print(\"Total de tokens únicos:\", len(set(tokens)))\n",
    "print(\"Tokens mais comuns:\", frequencia.most_common(50))\n",
    "\n",
    "\n",
    "frequencia.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef5b1cc",
   "metadata": {},
   "source": [
    "## Utilização do Gensim para processamento dos dados\n",
    "\n",
    "Pelo que eu estudei, o melhor é passar para o Word2Vec as frases inteiras ao invés de palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350cad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(processed_sentences)\n",
    "model_complete = Word2Vec(processed_sentences_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee04010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gravando o modelo\n",
    "# desta maneira, posso acelerar o processamento, se necessário\n",
    "model.save(\"model.bin\")\n",
    "model_complete.save(\"model_complete.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bb9296",
   "metadata": {},
   "source": [
    "Eu reparei que muitas palavras terminadas na letra \"e\" estão sem a letra \"e\".\n",
    "Veja movie ou storie logo na primeira linha.\n",
    "Preciso entender qual transformação está retirando o \"e\" final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5095099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize vocabulary\n",
    "words = list(model.wv.index_to_key)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b240881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access vector for one word\n",
    "model.wv['movi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aba2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifica a similaridade\n",
    "model.wv.similarity(\"film\" , \"movi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5691002",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=[\"film\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f8f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantidade mínima de palavras\n",
    "tamanho_avaliacao = []\n",
    "for words, label in documents:\n",
    "    words_in_model = [word for word in words if word in model.wv]\n",
    "    tamanho_avaliacao.append(len(words_in_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a35edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(np.mean(tamanho_avaliacao))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24ee703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to feature vectors using Word2Vec embeddings\n",
    "def text_to_features(documents, model, feature_size=0):\n",
    "\n",
    "    # Preparar dados para o classificador\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    label_map = {'neg': 0, 'pos': 1}\n",
    "\n",
    "    for words, label in documents:\n",
    "        word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    \n",
    "        if len(word_vectors) > 0:\n",
    "            if feature_size == 0:\n",
    "                document_vector = np.mean(word_vectors, axis=0)\n",
    "            else:\n",
    "                if len(word_vectors) > feature_size:\n",
    "                    #trunca o word_vectors para o tamanho do vector_size\n",
    "                    document_vector_tmp = word_vectors[:feature_size-1]\n",
    "                else:\n",
    "                    document_vector_tmp = word_vectors\n",
    "                    #adiciona entradas 0 para completar o tamanho do vector_size\n",
    "                    for i in range(len(word_vectors),feature_size):\n",
    "                        document_vector_tmp.append([0]*model.wv.vector_size)\n",
    "\n",
    "                new_shape = (document_vector_tmp[0], document_vector_tmp[1] * document_vector_tmp[2])\n",
    "\n",
    "                # Reshape the array\n",
    "                # transforma de 2D em uma lista flat\n",
    "                document_vector = [item for sublist in document_vector_tmp for item in sublist]\n",
    "                        \n",
    "            X.append(document_vector)\n",
    "            y.append(label_map[label])\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1fcf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treina_avalia_modelo(document, model, feature_size=0):\n",
    "    X , y = text_to_features(processed_docs, model, feature_size)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "    xgb_classifier = xgb.XGBClassifier()\n",
    "    xgb_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "\n",
    "    # Print a classification report for more detailed evaluation\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return xgb_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8346f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avaliando modelo sem stop words e utilizando a média no Word2Vec\")\n",
    "treina_avalia_modelo(processed_docs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8db6d4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaliando modelo sem stop words e utilizando as primeiras 1582 entradas do Word2Vec como features\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11948\\1676553467.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Avaliando modelo sem stop words e utilizando as primeiras 1582 entradas do Word2Vec como features\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtreina_avalia_modelo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1582\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11948\\1631795651.py\u001b[0m in \u001b[0;36mtreina_avalia_modelo\u001b[1;34m(document, model, feature_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtreina_avalia_modelo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_to_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mxgb_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mxgb_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11948\\335400489.py\u001b[0m in \u001b[0;36mtext_to_features\u001b[1;34m(documents, model, feature_size)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[1;31m# Reshape the array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[0mdocument_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument_vector_tmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "print(\"Avaliando modelo sem stop words e utilizando as primeiras 1582 entradas do Word2Vec como features\")\n",
    "treina_avalia_modelo(processed_docs, model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7a52061a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape:\n",
      "(3, 2, 2)\n",
      "Reshaped Array:\n",
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example 3D array\n",
    "original_array = np.array([[[1, 2],\n",
    "                            [3, 4]],\n",
    "                           \n",
    "                           [[5, 6],\n",
    "                            [7, 8]],\n",
    "                           \n",
    "                           [[9, 10],\n",
    "                            [11, 12]]])\n",
    "\n",
    "# Get the original shape\n",
    "original_shape = original_array.shape\n",
    "\n",
    "# Calculate the new shape by combining the second and third dimensions\n",
    "new_shape = (original_shape[0], original_shape[1] * original_shape[2])\n",
    "\n",
    "# Reshape the array\n",
    "reshaped_array = original_array.reshape(new_shape)\n",
    "\n",
    "print(\"Original Shape:\")\n",
    "print(original_shape)\n",
    "print(\"Reshaped Array:\")\n",
    "print(reshaped_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "81a86c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
